\newcommand{\timebound}{4000}
\newcommand{\query}{[INSERIRE LA QUERY]}

\section{Properties}
In this section we are going to describe the properties that we have verified over the system to ensure its correctness and to analyse its behaviour. Unless otherwise specified, we ran our verifications with a time bounded to \timebound \space time units. We set this bound after observing the behaviour of a medium-configuration system when changing the time bounded. We concluded that \timebound \space should be the correct number of time units to ensure a relevant bounded model checking.

During the verification phase we have used the default statistical parameters of UPPAAL. It was reasonable considering a confidence of 95\% for verification purposes. We also considered the other statistical parameters as reasonable. Those parameters and those values are reported in the Table \ref{tab:statparam}.

\begin{table}[h]
    \centering
        \begin{tabular}{|c c|} 
            \hline
            Parameter & Value \\ [0.5ex] 
            \hline\hline
            $\pm\delta$ & 0.01 \\
            $\beta$ & 0.05 \\
            $\alpha$ & 0.01 \\
            $\epsilon$ & 0.05 \\
            $\mu_0$ & 0.9 \\
            $\mu_1$ & 1.1 \\
            Trace resolution & 4.096 \\
            Discretization step & 0.01 \\ [0.5ex] 
            \hline
        \end{tabular}
        \caption{Statistical parameters of UPPAAL}
        \label{tab:statparam}
\end{table}

\subsection{Full queue}
This is the most important property to be verified, in order to understand if the system reacts in a proper way to several configurations. The system should sustain a high number of tasts generated during the execution.

In this section we will stress the system changing the probability distribution of some delays and modifying the configuration of the grid. We will also try adding some robots into the system, so that we will be able to see the main differences, comparing them and understanding what are the trade-off to take into account. In particular, the parameters which I will refer to are listed here:
\begin{itemize}
    \item ${mT}$: the average delay of the task generation
    \item ${vT}$: the standard deviation of the task generation
    \item ${mH}$: the average delay when a human picks an item
    \item ${vH}$: the standard deviation when a human picks an item
    \item ${k}$: the \emph{deterministic} value of the time that passes between each move of the robot
\end{itemize}

To check if the queue had run out of space, we used the following query. \query

We analysed three different scenarios. With the first 2, we analysed the behaviour of the system with a grid \texttt{10x10}, then we observed the reaction of the system when using a different configuration, moving the dimensions from \texttt{10x10} to \texttt{15x15}.

\subsubsection{First Scenario}
This is the simplest case. In this scenario, there were three robots and the dimension of the grid was \texttt{10x10}. At Table \ref{tab:firstattempt} we reported the parameters that we chose in the first attempt.

\begin{table}[h]
\centering
    \begin{tabular}{|c c|} 
        \hline
        Parameter & Value \\ [0.5ex] 
        \hline\hline
        mT & 40 \\
        vT & 5 \\
        mH & 3 \\
        vH & 2 \\
        k & 1 \\ [0.5ex] 
        \hline
    \end{tabular}
    \caption{First attempt}
    \label{tab:firstattempt}
\end{table}

Given that three robots are not able to handle a large number of tasks, we chose to give a high average time for the task generation delays while maintaining a general realistic behaviour between the robots and the human. We assumed that the human is slower than the robots and that it has a significant high deviation standard (with respect to its own average time). We also assumed the task generation to be quite constant (the deviation standard is about the 12 percent of the average time), in order to avoid creating a situation too floating.

The aforementioned assumptions could bring to the conclusion that, probably, the robots will be able to handle the tasks generated quite easily. However, the verification results show that there is about the 30\% of probability that the queue will reach the maximum size. More precisely, after 338 runs, the tool can affirm with a confidence of the 95\% that the queue will end up full with a probability within the interval \texttt{[0.247697,0.347638]}.

As a consequence, we decreased the task generation frequency until the result reached a low probability. We focused on the average delay time $mT$ and we changed the other parameters only in order to give a realistic behaviour to the overall system. With the values indicated at Table \ref{tab:lowbound}, the probability of running out of space was in the interval \texttt{[0.0413775,0.141115]}. We considered it sufficient to be considered a successful result.

\begin{table}[h]
    \centering
        \begin{tabular}{|c c|} 
            \hline
            Parameter & Value \\ [0.5ex] 
            \hline\hline
            mT & 150 \\
            vT & 10 \\
            mH & 6 \\
            vH & 4 \\
            k & 1 \\ [0.5ex] 
            \hline
        \end{tabular}
        \caption{Low bound}
        \label{tab:lowbound}
\end{table}

We also tried rising the resulting probability with the aim of analysing the behaviour of the system under stress, with few robots. We did it in three ways:
\begin{itemize}
    \item Firstly, we tried increasing $k$, with the aim of delaying the robots. It was enough to double the delay in order to reach the maximum probabilistic range, considering the tool's parameters which we mentioned above. With the values specified at Table \ref{tab:upboundrobot}, the resulting probability of exceeding the maximum queue size is \texttt{[0.901855,1]}.
    \item Subsequently we tried decreasing the task generation delay. With the values described at Table \ref{tab:upboundtask}, we already obtained a probability in the range of \texttt{[0.763922,0.863898]}.
    \item Finally, we modified the previous analysis, delaying, a little, also the human (Table \ref{tab:upboundhuman}). In this case the probability showed this interval: \texttt{[0.901855,1]}.
\end{itemize}

[VOGLIO LE TABELLE UNA DI FIANCO ALL'ALTRA]

\begin{table}[h]
    \centering
        \begin{tabular}{|c c|} 
            \hline
            Parameter & Value \\ [0.5ex] 
            \hline\hline
            mT & 40 \\
            vT & 5 \\
            mH & 6 \\
            vH & 4 \\
            k & 2 \\ [0.5ex] 
            \hline
        \end{tabular}
        \caption{Upper bound modifying $k$}
        \label{tab:upboundrobot}
\end{table}

\begin{table}[h]
    \centering
        \begin{tabular}{|c c|} 
            \hline
            Parameter & Value \\ [0.5ex] 
            \hline\hline
            mT & 40 \\
            vT & 5 \\
            mH & 6 \\
            vH & 4 \\
            k & 2 \\ [0.5ex] 
            \hline
        \end{tabular}
        \caption{Upper bound modifying $mT$}
        \label{tab:upboundtask}
\end{table}

\begin{table}[h]
    \centering
        \begin{tabular}{|c c|} 
            \hline
            Parameter & Value \\ [0.5ex] 
            \hline\hline
            mT & 40 \\
            vT & 5 \\
            mH & 6 \\
            vH & 4 \\
            k & 2 \\ [0.5ex] 
            \hline
        \end{tabular}
        \caption{Upper bound modifying $mT$}
        \label{tab:upboundhuman}
\end{table}

We have analysed a scenario with few robots and a \texttt{10x10} grid.

We would like to focus on the values used in order to make the verification mentioned above. As you can notice looking at the Tables \ref{tab:firstattempt}, \ref{tab:lowbound}, \ref{tab:upboundrobot}, \ref{tab:upboundtask} and \ref{tab:upboundhuman}, it is easier to stress the system rather than to have a suitable configuration. [NEL CASO SI AVVERI, DIRO' CHE QUESTO E' DOVUTO AI POCHI ROBOT E CHE SI PUO' VEDERE LA DIFFERENZA CON GLI STESSI PARAMETRI USATI DA PIU' ROBOT]

\subsubsection{Second Scenario}
In this scenario, we will analyse a configurations with six robots and a \texttt{10x10} grid.

The intuitive reasoning could be that the more robots there are in the system, the faster it will in digesting the tasks continuously generated. Moreover, the number of robots is much less than the number of possible cells in the grid.

However, a \texttt{10x10} grid with the configuration showed before [DO PER SCONTATO CHE ABBIATE MOSTRATO O CITATO LA BOARD] is not enough to avoid `traffic' among the robots. Indeed, the results of the verification are far worser than the expected. In order to compare the two scenarios, we used the same values of Table \ref{tab:firstattempt} and observed the results. The probability of filling up the queue is greater than before: it lies in the range \texttt{[0.601786,0.701727]}.

We therefore did the same steps as in the previous scenarios, to see if there were some differences in the distribution of the probabilities. After using the same values of Tabel \ref{tab:lowbound}, the resulting range of probability was \texttt{[0.199284,0.299149]}, about the 15\% more than the result with three robots. Considering that the verification results showed in the previous paragraph were about the 35\% higher than the case with three robots, we can conclude that with more robots it is faster improving the situation. It also makes sense, because the less tasks there are, the less `traffic' there is.

In order to reach about the same interval of probabilities, we also ran a verification with $mT = 200$ and, producing a resulting interval of \\ \texttt{[0.0619394,0.16173]}.

\subsubsection{Third Scenario}
The last scenario that we would like to present is less specific. We stressed an environment with a \texttt{9x9} grid and 4 robots, in order to combine together some properties.

Here we concentrated more on the reactions of the system when changing the standard deviations and the human average delay time.

We will only report the relevant results.

In questo pezzo ho fatto una prova normale, poi ho aumentato la deviazione standard prima del task e poi del human. In entrambi i casi, va a incidere solo se superi il valore della media, e va incidere poco. Comunque, per l'umano aumenta la probabiiltà di errore e invece per il task diminusice, giustamente.

Mi sveglio tra 3 ore e mezza e finiamo. Ho vomitato tutto ciò che c'è da scrivere, almeno da quel punto di vista ho finito, poi tocca abbellirlo.

Inoltre, non sono riuscito a fare altre proprietà non tanto per mancanza di tempo, quanto perché non sono riuscito proprio neanche a pensare a cose sensate (e la cosa del pods\_unavailable non mi funzionava)